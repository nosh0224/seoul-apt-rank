import os
import sys
import argparse
import requests
import pandas as pd
import datetime
import json
import webbrowser
import time
import xml.etree.ElementTree as ET
from dotenv import load_dotenv
from tqdm import tqdm
from flask import Flask, render_template, jsonify, request
from threading import Timer
import threading
import traceback
from concurrent.futures import ThreadPoolExecutor, as_completed
from dateutil.relativedelta import relativedelta

# Load environment variables
load_dotenv()

# Configuration
API_KEY = os.getenv("MOLIT_API_KEY")
API_URL = "https://apis.data.go.kr/1613000/RTMSDataSvcAptTradeDev/getRTMSDataSvcAptTradeDev"
CSV_FILENAME = "seoul_apt_ranking.csv"
RAW_CSV_FILENAME = "seoul_apt_trades.csv"

# Seoul District Codes
SEOUL_DISTRICTS = {
    "11110": "ì¢…ë¡œêµ¬", "11140": "ì¤‘êµ¬", "11170": "ìš©ì‚°êµ¬", "11200": "ì„±ë™êµ¬",
    "11215": "ê´‘ì§„êµ¬", "11230": "ë™ëŒ€ë¬¸êµ¬", "11260": "ì¤‘ë‘êµ¬", "11290": "ì„±ë¶êµ¬",
    "11305": "ê°•ë¶êµ¬", "11320": "ë„ë´‰êµ¬", "11350": "ë…¸ì›êµ¬", "11380": "ì€í‰êµ¬",
    "11410": "ì„œëŒ€ë¬¸êµ¬", "11440": "ë§ˆí¬êµ¬", "11470": "ì–‘ì²œêµ¬", "11500": "ê°•ì„œêµ¬",
    "11530": "êµ¬ë¡œêµ¬", "11545": "ê¸ˆì²œêµ¬", "11560": "ì˜ë“±í¬êµ¬", "11590": "ë™ì‘êµ¬",
    "11620": "ê´€ì•…êµ¬", "11650": "ì„œì´ˆêµ¬", "11680": "ê°•ë‚¨êµ¬", "11710": "ì†¡íŒŒêµ¬",
    "11740": "ê°•ë™êµ¬"
}

# Initialize Flask App (Global Scope for Gunicorn)
app = Flask(__name__, template_folder='templates')

# Global Status for Background Task
UPDATE_STATUS = {'running': False, 'message': ''}

def fetch_data(lawd_cd, deal_ymd):
    if not API_KEY: 
        print(f"Error: API Key is missing", flush=True)
        return None
    full_url = f"{API_URL}?serviceKey={API_KEY}&LAWD_CD={lawd_cd}&DEAL_YMD={deal_ymd}&numOfRows=9999&pageNo=1"
    try:
        response = requests.get(full_url, timeout=30)
        response.encoding = 'utf-8'
        if response.status_code != 200:
            print(f"[API Error] Status: {response.status_code} for {lawd_cd}/{deal_ymd}", flush=True)
            return None
        return response.text
    except Exception as e:
        print(f"[API Exception] {lawd_cd}/{deal_ymd}: {e}", flush=True)
        return None

def parse_xml_to_df(xml_data, district_name):
    if not xml_data: return pd.DataFrame()
    try:
        root = ET.fromstring(xml_data)
        items = root.findall(".//item")
        if not items: return pd.DataFrame()
        data = []
        for item in items:
            try:
                row = {
                    "ìì¹˜êµ¬": district_name,
                    "ë²•ì •ë™": (item.findtext("umdNm") or item.findtext("aptDong") or "").strip(),
                    "ì•„íŒŒíŠ¸": (item.findtext("aptNm") or "").strip(),
                    "ê±°ë˜ê¸ˆì•¡": int(item.findtext("dealAmount").strip().replace(",", "")) if item.findtext("dealAmount") else 0,
                    "ë…„": item.findtext("dealYear"),
                    "ì›”": item.findtext("dealMonth"),
                    "ì¼": item.findtext("dealDay"),
                    "ì „ìš©ë©´ì ": float(item.findtext("excluUseAr")) if item.findtext("excluUseAr") else 0.0,
                    "ì¸µ": item.findtext("floor") or "0",
                }
                if row["ë…„"] and row["ì›”"] and row["ì¼"]:
                    row["ê±°ë˜ì¼ì"] = f"{row['ë…„']}-{row['ì›”'].zfill(2)}-{row['ì¼'].zfill(2)}"
                else: row["ê±°ë˜ì¼ì"] = "2000-01-01"
                data.append(row)
            except: continue
        return pd.DataFrame(data)
    except: return pd.DataFrame()

def get_price_tier(price):
    if price < 100000: return "10ì–µ ë¯¸ë§Œ"
    elif price < 150000: return "10ì–µ~15ì–µ"
    elif price < 200000: return "15ì–µ~20ì–µ"
    else: return "20ì–µ ì´ìƒ"

def get_area_tier(area):
    if area < 50: return 10
    elif area < 70: return 20
    elif area < 102: return 30
    elif area < 135: return 40
    else: return 50

def analyze_data(df):
    if df.empty: return pd.DataFrame()
    grouped = df.groupby(["ìì¹˜êµ¬", "ë²•ì •ë™", "ì•„íŒŒíŠ¸", "ë…„", "ì „ìš©ë©´ì "]).agg(
        ê±°ë˜ê±´ìˆ˜=("ê±°ë˜ê¸ˆì•¡", "count"),
        í‰ê· ê±°ë˜ê¸ˆì•¡=("ê±°ë˜ê¸ˆì•¡", "mean"),
        ìµœê·¼ê±°ë˜ì¼=("ê±°ë˜ì¼ì", "max")
    ).reset_index()
    grouped = grouped.sort_values(by="ê±°ë˜ê±´ìˆ˜", ascending=False)
    grouped["ê°€ê²©ëŒ€"] = grouped["í‰ê· ê±°ë˜ê¸ˆì•¡"].apply(get_price_tier)
    grouped["í‰í˜•ëŒ€"] = grouped["ì „ìš©ë©´ì "].apply(get_area_tier)
    return grouped

def collect_and_save_data():
    print(">>> Starting Data Collection...", flush=True)
    end_date = datetime.date.today()
    start_date = end_date - relativedelta(years=3)
    months = []
    curr = start_date
    while curr <= end_date:
        months.append(curr.strftime("%Y%m"))
        curr += relativedelta(months=1)
    
    tasks = []
    for district_code, district_name in SEOUL_DISTRICTS.items():
        for m in months: tasks.append((district_code, district_name, m))
            
    print(f">>> Total Tasks to fetch: {len(tasks)}", flush=True)
    all_dfs = []
    completed = 0
    with ThreadPoolExecutor(max_workers=12) as executor:
        future_to_task = {executor.submit(fetch_data, c, y): (c, n, y) for c, n, y in tasks}
        for future in as_completed(future_to_task):
            completed += 1
            if completed % 100 == 0:
                print(f">>> Progress: {completed}/{len(tasks)} ({completed/len(tasks)*100:.1f}%)", flush=True)
            
            _, name, _ = future_to_task[future]
            try:
                xml_txt = future.result()
                if xml_txt:
                    df = parse_xml_to_df(xml_txt, name)
                    if not df.empty: all_dfs.append(df)
            except Exception as e: 
                print(f"[Task Error] {name}: {e}", flush=True)

    print(f">>> Fetching Finished. Blocks found: {len(all_dfs)}", flush=True)
    if not all_dfs: return False, "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: API ì‘ë‹µì´ ì—†ê±°ë‚˜ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ì…ë‹ˆë‹¤."
    try:
        full_df = pd.concat(all_dfs, ignore_index=True)
        print(f">>> Saving raw data ({len(full_df)} rows)...", flush=True)
        full_df.to_csv(RAW_CSV_FILENAME, index=False, encoding='utf-8-sig')
        
        print(">>> Analyzing & Saving ranking data...", flush=True)
        analyzed_df = analyze_data(full_df)
        analyzed_df.to_csv(CSV_FILENAME, index=False, encoding='utf-8-sig')
        print(">>> All Data Processes Completed Successfully!", flush=True)
        return True, f"ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(full_df)}ê±´ì˜ ê±°ë˜ ë°ì´í„° ë¶„ì„ë¨."
    except Exception as e: 
        print(f"[Save Error] {e}", flush=True)
        return False, f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"

def validate_data_file():
    if not os.path.exists(CSV_FILENAME): return False
    try:
        df = pd.read_csv(CSV_FILENAME)
        return not df.empty and 'ë…„' in df.columns
    except: return False

@app.route('/')
def index():
    if not validate_data_file():
        return '''<div style="text-align:center; padding:50px; font-family:sans-serif;">
            <h1>ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.</h1>
            <p>ì„œë²„ì—ì„œ ìˆ˜ì§‘ì„ ì‹œì‘í•´ì£¼ì„¸ìš”. (ì•½ 3~5ë¶„ ì†Œìš”)</p>
            <button onclick="this.disabled=true; this.innerText='ìˆ˜ì§‘ ì‹œì‘ë¨...'; fetch('/update', {method:'POST'})" 
                    style="padding:15px 30px; cursor:pointer; background:#4F46E5; color:white; border:none; border-radius:8px;">
                ğŸ”„ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘
            </button>
        </div>'''
    
    try:
        mtime = os.path.getmtime(CSV_FILENAME)
        last_updated = datetime.datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M')
        df = pd.read_csv(CSV_FILENAME).fillna(0)
        records = df.to_dict('records')
        years_list = sorted(df['ë…„'].unique().tolist())
        district_list = sorted(list(SEOUL_DISTRICTS.values()))
        return render_template('index.html', data_json=json.dumps(records), years=years_list, districts=district_list, last_updated=last_updated)
    except Exception as e: return f"Error: {str(e)}"

@app.route('/update', methods=['POST'])
def update_data():
    global UPDATE_STATUS
    if UPDATE_STATUS['running']:
        return jsonify({'status': 'error', 'message': 'ì´ë¯¸ ë°ì´í„° ìˆ˜ì§‘ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.'})

    def task():
        global UPDATE_STATUS
        try:
            UPDATE_STATUS['running'] = True
            UPDATE_STATUS['message'] = 'ë°ì´í„° ìˆ˜ì§‘ ì¤‘...'
            print(">>> Background Update Started", flush=True)
            
            success, msg = collect_and_save_data()
            
            UPDATE_STATUS['running'] = False
            UPDATE_STATUS['message'] = 'success' if success else f'error: {msg}'
            print(f">>> Background Update Finished: {msg}", flush=True)
        except Exception as e:
            traceback.print_exc()
            UPDATE_STATUS['running'] = False
            UPDATE_STATUS['message'] = f'error: {str(e)}'

    thread = threading.Thread(target=task)
    thread.daemon = True 
    thread.start()
    return jsonify({'status': 'success', 'message': 'ìˆ˜ì§‘ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì™„ë£Œ ì‹œ ì•Œë¦¼ì´ ëœ¹ë‹ˆë‹¤.'})

@app.route('/update/status', methods=['GET'])
def get_update_status():
    return jsonify(UPDATE_STATUS)

@app.route('/api/data', methods=['GET'])
def get_data_api():
    if not validate_data_file(): return jsonify({'status': 'error'}), 404
    df = pd.read_csv(CSV_FILENAME).fillna(0)
    return jsonify({'status': 'success', 'data': df.to_dict('records')})

@app.route('/api/history', methods=['GET'])
def get_apt_history():
    apt_name = request.args.get('apt_name')
    dong = request.args.get('dong')
    if not os.path.exists(RAW_CSV_FILENAME): return jsonify({'status': 'error'}), 404
    df = pd.read_csv(RAW_CSV_FILENAME)
    mask = (df['ì•„íŒŒíŠ¸'] == apt_name) & (df['ë²•ì •ë™'] == dong)
    filtered = df[mask].fillna(0).sort_values(by='ê±°ë˜ì¼ì', ascending=False)
    return jsonify({'status': 'success', 'data': filtered.to_dict('records')})

if __name__ == "__main__":
    app.run(host='0.0.0.0', port=5000)